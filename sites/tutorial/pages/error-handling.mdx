export const meta = {
  "title": "Handling errors",
  "publishedOn": "January, 6th. 2022",
  "excerpt": "How to handle errors in tokenization",
  "snipenv": "v1",
}

# Error Handling

Errors can occur either due to invalid patterns or when no match is performed, eg:

```ts snippet=s1
import { Tokenizer, Rule, TapeInterface as Tape, Token } from "tlex";
const tokenizer = new Tokenizer()
                  .add("a")
                  .add("b")
                  .add("c")
                  .add("d+b")
                  .add(/\s+/, { skip: true } );
```

With the following input:

```ts snippet=s2 prev=s1 silent=true
console.log(tokenizer.tokenize("a b c ddddb e"));
```

We would expect an error at the "e" as it is not the start character of any rule.  Note that the error is returned as a token at the end.

<SnippOut src="s2" />

Similarly for a slightly different input:

```ts snippet=s3 prev=s1 silent=true
console.log(tokenizer.tokenize("a b c ddddcb e"));
```

we would expect the failure on the invalid lexeme `ddddc`:

<SnippOut src="s3" />

As before tokens before the error are returned with the error token being the last item.

## Catching errors

Instead of stopping at the first invalid character or lexeme, errors can be caught so that lexing can be continued (either via a reset, or user specified correction).

```ts snippet=catching_errors prev=s1
tokenizer.onError = (err: Error, tape: Tape, index: number) => {
  return null;
};
console.log(tokenizer.tokenize("a b c ddddcb e a b c"));
```

Here when an error is encountered it is simply skipped and tokenization continues at the next index.
