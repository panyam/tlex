export const meta = {
  "title": "Configuring rules",
  "publishedOn": "January, 6th. 2022",
  "excerpt": "Setup instructions on getting started with TLEX",
  "snipenv": "v1",
}

# Configuring Rules

## Actions

Rules are added with the `.add` method on the Tokenizer.  On their own just adding patterns is not very interesting.  Actions can also be attached to each token via the add method:

```ts snippet=s1 silent=true
import { Tokenizer, Rule, TapeInterface as Tape, Token } from "tlex";
const tokenizer = new Tokenizer()
  .add(/\d+/, (rule: Rule, tape: Tape, token: Token, owner: any) => {
    console.log("Found Token: ", token);
    token.value = parseInt(token.value);
    return token;
  })
  .add(/\w+/, (rule: Rule, tape: Tape, token: Token, owner: any) => {
    console.log("Found a word: ", token);
    token.value = token.value.toUpperCase();
    return token;
  })
  .add(/\s+/ , (rule: Rule, tape: Tape, token: Token, owner: any) => {
    console.log("Found a space: ", token);
    return null;
  });
const tokens = tokenizer.tokenize("123  hello  world");
```

Running the tokenizer above would execute the actions:

<SnippOut src="s1" />

In this example, the match handlers is a RuleMatchHandler.  These handler can be used to:

* Modify token values
* Filter out unwanted tokens
* Change the semantics of tokens (eg convert a integer string into an integer value).
* Change tokenizer states

With the tokenizer executing the actions we can print the resulting tokens:

```ts snippet=s4 prev=s1 silent=true
  console.log("All Tokens: ", tokens);
```

producing:

<SnippOut src="s4" />

## Skipping Tokens

Explicit handler returning null can be used to skip particular tokens.  Alternatively the "skip" parameter can be used:

```ts snippet=useSkip silent=true
import { Tokenizer, Rule, TapeInterface as Tape, Token } from "tlex";
const tokenizer = new Tokenizer()
    .add(/\d+/, (rule: Rule, tape: Tape, token: Token, owner: any) => {
      token.value = parseInt(token.value);
      return token;
    })
    .add(/\w+/, (rule: Rule, tape: Tape, token: Token, owner: any) => {
      token.value = token.value.toUpperCase();
      return token;
    })
    .add(/\s+/, {skip: true});
console.log(tokenizer.tokenize("123  hello  world"));
```

resulting in:

<SnippOut src="useSkip" />

## Tagging Tokens

Tokens can also be given custom tags so that they can referred by more meaningful labels instead of by their rules.

```ts snippet=s5 silent=true
import { Tokenizer, Rule, TapeInterface as Tape, Token } from "tlex";
const tokenizer = new Tokenizer()
    .add(/\d+/, {tag: "NUMBER"}, (rule: Rule, tape: Tape, token: Token, owner: any) => {
      token.value = parseInt(token.value);
      return token;
    })
    .add(/\w+/, {tag: "WORD"}, (rule: Rule, tape: Tape, token: Token, owner: any) => {
      token.value = token.value.toUpperCase();
      return token;
    })
    .add(/\s+/, {skip: true});
const tokens = tokenizer.tokenize("123  hello  world");
console.log("Tokens: ", tokens);
```

Running the tokenizer above would execute the actions:

<SnippOut src="s5" />

## Rule ordering and priorities

By default when rules are added to the tokenizer they are matched in the order in which they are specified.

For example in the following config:

```ts snippet=1 silent=true
  import * as TLEX from "tlex";
  const tokenizer = new TLEX.Tokenizer()
                  .add("hello")
                  .add("world")
                  .add(/\s+/)
                  .add(/h.*o/);
```

matching:

```ts snippet=2 prev=1 silent=true
  console.log(tokenizer.tokenize("hello hello"));
```

would yield the following tokens:

<SnippOut src="2" />

Notice the matchIndex - denoting the rule that matched.  Both (non-space) tokens have a matchIndex of 0 (instead of the longer 3 corresponding to "h.*o").

Rules can be given priorities so that those are matched ahead of rules with lower priorities (regardless of ordering).  Two rules that have equal priority will be matched in the order of addition, eg:

```ts snippet=priorities
  import * as TLEX from "tlex";
  const tokenizer = new TLEX.Tokenizer()
                  .add("hello")
                  .add(/\s+/)
                  .add(/h.*o/, {priority: 20});
  console.log(tokenizer.tokenize("hello hello"));
```

By default all rules have a priority of 10.
